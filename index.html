<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed Image Restoration">
  <meta name="keywords" content="MoE-Prompts">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed Image Restoration</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://lixinustc.github.io/projects/KVQ/">
            KVQ
          </a>
          <a class="navbar-item" href="https://wxrui182.github.io/CoNo.github.io/">
            CoNo
          </a>
          
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed Image Restoration</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=234Nza8AAAAJ">Yulin Ren</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=sbiY97gAAAAJ&hl=zh-CN">Xin Li</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=qHeWjNwAAAAJ&hl=en&authuser=1">Bingchen Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=cZEGgJ0AAAAJ&hl=zh-CN">Xingrui Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=6aQe5dUAAAAJ&hl=en">Mengxi Guo</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Shijie Zhao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=8G5-2OMAAAAJ&hl=zh-CN">Li Zhang</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Zhibo Chen</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">(ECCV 2024) </span>
        </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>2</sup>Bytedance Inc,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/renyulin-f/MoE-DiffIR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/15RNhqaOVUYWA6xy_sIw6fC8jDX1hLbL8/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" autoplay muted loop playsinline height="100%">
            <img src="./imgs/Figure1.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
      </img>
      <h2 class="subtitle has-text-centered">
        CIR Dataset
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Competition. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">CIR Dataset Information</h2>
        <div class="content has-text-justified">
          <p>
            We propose a comprehensive benchmark dataset for universal compressed image restoration (CIR), covering 21 types of degradations
            from 7 popular traditional and learned codecs. Traditional codecs includes JPEG</sup><a href="https://dl.acm.org/doi/abs/10.1145/103085.103089">[1]</a></sup>, VVC</sup><a href="https://ieeexplore.ieee.org/abstract/document/9503377">[2]</a></sup>, HEVC</sup><a href="https://link.springer.com/content/pdf/10.1007/978-3-319-06895-4.pdf">[3]</a></sup>, WEBP</sup><a href="https://developers.google.com/speed/webp/docs/compression?hl=zh-cn">[4]</a></sup>. Learned codecs includes HIFIC</sup><a href="https://arxiv.org/abs/2006.09965">[5]</a></sup>, \( C_{\text{SSIM}} \)</sup><a href="https://arxiv.org/abs/2001.01568">[6]</a></sup>, \( C_{\text{PSNR}} \)</sup><a href="https://arxiv.org/abs/2001.01568">[6]</a></sup>. Within each codec, we 
            apply three levels distortions:
            <ul>
                <li>JPEG: QF=10,15,20</li>
                <li>VVC: QP=37,42,47</li>
                <li>HEVC: QP=37,42,47</li>
                <li>WEBP: QF=1,5,10</li>
                <li>HIFIC: Mode='low', 'med', 'high'</li>
                <li>\( C_{\text{SSIM}} \): Mode=1,2,3</li>
                <li>\( C_{\text{PSNR}} \): Mode=1,3,3</li>
            </ul>
            Our CIR dataset is based on DF2K</sup><a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/html/Agustsson_NTIRE_2017_Challenge_CVPR_2017_paper.html">[7]</a></sup>, with the original DF2K images considered as the ground truth. All compression codecs are applied to these images. Here we release the <a href="https://drive.google.com/file/d/15RNhqaOVUYWA6xy_sIw6fC8jDX1hLbL8/view?usp=sharing">datalink</a> of CIR dataset including training datasets and tests datasets.
            The branch of this dataset is shown in following figure. The dataset comprises training and testing sets. In the training set, "DF2K_HR" contains the ground truth (GT) images, while the other folders correspond to 21 types of low-quality (LQ) images resulting from various compression distortions. The five testing sets also include folders corresponding to the 21 types of compressed LQ images, as well as folders for the high-resolution (HR) images.
          </p>
            <img src="./imgs/Data_branch.png"
                 class="interpolation-image"
                 alt="competition."/>
          <p>
            We also release unseen tasks testsets apart from CIR dataset. It consists of "Cross Degrees" and "Cross Types". We choose JPEG code from QF value [5,25]
            and select AVC(H.264) codec from vaules [47,42,37]. The branch is shown in following figure.         
          </p>
          <img src="./imgs/Data_branch2.png"
                 class="interpolation-image"
                 alt="competition."/>
          </div>
        </div>
      </div>
    </div>
    <!--/ Competition. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present MoE-DiffIR, an innovative universal compressed image restoration (CIR) method with task-customized diffusion priors. This intends to handle two pivotal challenges in the existing CIR methods: (i) lacking adaptability and universality for different image codecs, e.g., JPEG and WebP; (ii) poor texture generation capability, particularly at low bitrates. Specifically, our MoE-DiffIR develops the powerful mixture-of-experts (MoE) prompt module, where some basic prompts cooperate to excavate the task-customized diffusion priors from Stable Diffusion (SD) for each compression task. Moreover, the degradation-aware routing mechanism is proposed to enable the flexible assignment of basic prompts. To activate and reuse the cross-modality generation prior of SD, we design the visual-to-text adapter for MoE-DiffIR, which aims to
            adapt the embedding of low-quality images from the visual domain to the textual domain as the textual guidance for SD, enabling more consistent and reasonable texture generation. We also construct one comprehensive benchmark dataset for universal CIR, covering 21 types of degradations from 7 popular traditional and learned codecs. Extensive experiments on universal CIR have demonstrated the excellent robustness and texture restoration capability of our proposed MoE-DiffIR.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>




</section>

<!-- Concurrent Work. -->
<div class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Reference</h2>

    <div class="content has-text-justified">
      <p>
        [1] Wallace, G.K.: The jpeg still picture compression standard. Communications of the ACM 34(4), 30–44 (1991)
      </p>
      <p>
        [2] Bross, B., Wang, Y.K., Ye, Y., Liu, S., Chen, J., Sullivan, G.J., Ohm, J.R.: Overview of the versatile video coding (vvc) standard and its applications. IEEE Transactions on Circuits and Systems for Video Technology 31(10), 3736–3764 (2021)
      </p>
      <p>
        [3] Sze, V., Budagavi, M. and Sullivan, G.J., 2014. High efficiency video coding (HEVC). In Integrated circuit and systems, algorithms and architectures (Vol. 39, p. 40). Berlin, Germany: Springer.
      </p>
      <p>
        [4] Ginesu, G., Pintus, M., Giusto, D.D.: Objective assessment of the webp image coding algorithm. Signal processing: image communication 27(8), 867–874 (2012)
      </p>
      <p>
        [5] Mentzer, F., Toderici, G.D., Tschannen, M. and Agustsson, E., 2020. High-fidelity generative image compression. Advances in Neural Information Processing Systems, 33, pp.11913-11924.
      </p>
      <p>
        [6] Cheng, Z., Sun, H., Takeuchi, M. and Katto, J., 2020. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7939-7948).
      <p>
        [7] Agustsson, E., Timofte, R.: Ntire 2017 challenge on single image super-resolution: Dataset and study. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 126–135 (2017)
      </p>
    </div>
  </div>
</div>
<!--/ Concurrent Work. -->

</div>
</section>






<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{ren2024MoE-DiffIR,
        title={MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed Image Restoration},
        author={Ren, Yulin and Li, Xin and Li, Bingchen and Wang, Xingrui and Guo, Mengxi and Zhao, Shijie and Zhang, Li and Chen, Zhibo},
        booktitle={European Conference on Computer Vision},
        year={2024},
        organization={Springer}
      }
    </code></pre>
  </div>
</section>







<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2402.07220.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/lixinustc" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
